{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperToken Python Bridge Demo\n",
    "\n",
    "This notebook demonstrates using HyperToken environments in Python through the WebSocket bridge.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Start the HyperToken server before running this notebook:\n",
    "\n",
    "```bash\n",
    "npx tsx bridge/server.ts --env blackjack --port 9999 --verbose\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install hypertoken if needed\n",
    "# !pip install hypertoken\n",
    "\n",
    "from hypertoken import HyperTokenAECEnv, HyperTokenClient\n",
    "import numpy as np\n",
    "\n",
    "# Connect to the server\n",
    "env = HyperTokenAECEnv(\"ws://localhost:9999\")\n",
    "\n",
    "print(f\"Connected to HyperToken!\")\n",
    "print(f\"Possible agents: {env.possible_agents}\")\n",
    "print(f\"Observation space: {env.observation_space(env.possible_agents[0])}\")\n",
    "print(f\"Action space: {env.action_space(env.possible_agents[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check latency to server\n",
    "client = HyperTokenClient(\"ws://localhost:9999\")\n",
    "client.connect()\n",
    "\n",
    "latencies = [client.ping() for _ in range(10)]\n",
    "print(f\"Average latency: {np.mean(latencies):.2f}ms\")\n",
    "print(f\"Min latency: {np.min(latencies):.2f}ms\")\n",
    "print(f\"Max latency: {np.max(latencies):.2f}ms\")\n",
    "\n",
    "client.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running a Single Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "env.reset(seed=42)\n",
    "\n",
    "print(\"Episode started!\")\n",
    "print(f\"Active agents: {env.agents}\")\n",
    "print(f\"Current agent: {env.agent_selection}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through the episode\n",
    "step_count = 0\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    obs, reward, terminated, truncated, info = env.last()\n",
    "    \n",
    "    print(f\"\\n--- Step {step_count} ---\")\n",
    "    print(f\"Agent: {agent}\")\n",
    "    print(f\"Observation: {obs}\")\n",
    "    print(f\"Reward: {reward}\")\n",
    "    print(f\"Terminated: {terminated}, Truncated: {truncated}\")\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        print(\"Agent done, passing None action\")\n",
    "        action = None\n",
    "    else:\n",
    "        # Get valid actions\n",
    "        mask = env.action_mask(agent)\n",
    "        print(f\"Action mask: {mask}\")\n",
    "        \n",
    "        # Sample a random valid action\n",
    "        action = env.action_space(agent).sample()\n",
    "        if mask is not None and not mask[action]:\n",
    "            valid_actions = np.where(mask)[0]\n",
    "            action = np.random.choice(valid_actions)\n",
    "        \n",
    "        action_names = ['Hit', 'Stand', 'Double', 'Split', 'Insurance']\n",
    "        print(f\"Taking action: {action} ({action_names[action]})\")\n",
    "    \n",
    "    env.step(action)\n",
    "    step_count += 1\n",
    "    \n",
    "    if step_count > 20:  # Safety limit\n",
    "        print(\"\\nReached step limit, breaking...\")\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Episode complete!\")\n",
    "print(f\"Final rewards: {env.rewards()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running Multiple Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy='random'):\n",
    "    \"\"\"Run a single episode with the given policy.\"\"\"\n",
    "    env.reset()\n",
    "    \n",
    "    for agent in env.agent_iter():\n",
    "        obs, reward, term, trunc, info = env.last()\n",
    "        \n",
    "        if term or trunc:\n",
    "            action = None\n",
    "        else:\n",
    "            mask = env.action_mask(agent)\n",
    "            \n",
    "            if policy == 'random':\n",
    "                action = env.action_space(agent).sample()\n",
    "            elif policy == 'conservative':\n",
    "                # Conservative: stand if hand value > 16\n",
    "                hand_value = obs[0] * 30  # Denormalize\n",
    "                action = 1 if hand_value > 16 else 0  # Stand or Hit\n",
    "            else:\n",
    "                action = env.action_space(agent).sample()\n",
    "            \n",
    "            # Ensure action is valid\n",
    "            if mask is not None and not mask[action]:\n",
    "                valid = np.where(mask)[0]\n",
    "                action = np.random.choice(valid) if len(valid) > 0 else 1\n",
    "        \n",
    "        env.step(action)\n",
    "    \n",
    "    return env.rewards()\n",
    "\n",
    "# Run 50 episodes\n",
    "num_episodes = 50\n",
    "all_rewards = {agent: [] for agent in env.possible_agents}\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    rewards = run_episode(env, policy='conservative')\n",
    "    for agent, r in rewards.items():\n",
    "        all_rewards[agent].append(r)\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Completed {i + 1}/{num_episodes} episodes\")\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for agent in env.possible_agents:\n",
    "    rewards = all_rewards[agent]\n",
    "    print(f\"  {agent}: avg={np.mean(rewards):.2f}, total={sum(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = ['random', 'conservative']\n",
    "results = {}\n",
    "\n",
    "for policy in policies:\n",
    "    rewards = []\n",
    "    for _ in range(30):\n",
    "        episode_rewards = run_episode(env, policy=policy)\n",
    "        # Sum rewards for first agent only\n",
    "        rewards.append(episode_rewards[env.possible_agents[0]])\n",
    "    \n",
    "    results[policy] = {\n",
    "        'mean': np.mean(rewards),\n",
    "        'std': np.std(rewards),\n",
    "        'total': sum(rewards)\n",
    "    }\n",
    "\n",
    "print(\"Policy Comparison (30 episodes each):\")\n",
    "print(\"-\" * 50)\n",
    "for policy, stats in results.items():\n",
    "    print(f\"{policy:15} mean={stats['mean']:7.2f} std={stats['std']:6.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the environment when done\n",
    "env.close()\n",
    "print(\"Environment closed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
